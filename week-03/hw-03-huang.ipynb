{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 03\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⚠️ Before you start ⚠️\n",
    "\n",
    "_Duplicate this Jupyter Notebook in your `week-03` folder (right-click -> Duplicate) and then add your last name to the beginning of it (ie. `hw-03-blevins.ipynb` - otherwise you risk having all your work overwritten when you try to sync your GitHub repository with your instructor's repository._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ _No, seriously: check the name of this file. Is it the copy you made? (ie. `hw-03-blevins.ipynb`). If so, you can proceed_ ⚠️\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "Student Name: Jiawen Huang\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're going to analyze several historical documents in this homework. In keeping with the theme of our first unit for the semester, **Slavery and Data**, I've chosen two 19th-century narratives written by formerly enslaved people: [Sojourner Truth](https://archive.org/details/narrativeofsojou1850gilb) and [Henry \"Box\" Brown](https://archive.org/details/narrativeofhenry00brow).\n",
    "\n",
    "You should have the following files:\n",
    "\n",
    "- `hw-03-yourlastname.ipynb` (your working version Jupyter Notebook)\n",
    "- `truth.txt` (Sojourner Truth's narrative)\n",
    "- `brown.txt` (Henry Brown's narrative)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Process the Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `open()` and `read()` functions to get the content of each of these files into Python, assigning them the corresponding variable names of `truth_fulltext` and `brown_fulltext`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_fulltext = open('truth.txt', mode='r', encoding='utf-8' ).read()\n",
    "brown_fulltext = open('brown.txt', mode='r', encoding='utf-8' ).read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next two code cells, write print() statements that:\n",
    "\n",
    "- Print the **first 500 characters** of Truth's narrative.\n",
    "- Print characters **5000 to 6000** of Brown's narrative.\n",
    "\n",
    "Hint: use the index and slice approaches for strings: <https://melaniewalsh.github.io/Intro-Cultural-Analytics/02-Python/06-String-Methods.html>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NARRATIVE OF SOJOURNER TRUTH\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "HER BIRTH AND PARENTAGE.\n",
      "\n",
      "\n",
      "THE subject of this biography, SOJOURNER TRUTH, as she now calls\n",
      "herself-but whose name, originally, was Isabella-was born, as near as\n",
      "she can now calculate, between the years 1797 and 1800.  She was the\n",
      "daughter of James and Betsey, slaves of one Colonel Ardinburgh, Hurley,\n",
      "Ulster County, New York.\n",
      "\n",
      "Colonel  Ardinburgh belonged to that class of people called Low Dutch.\n",
      "\n",
      "\n",
      "Of her first master, she can give no account, as she must have be\n"
     ]
    }
   ],
   "source": [
    "print(truth_fulltext[0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of pity, indignation and\n",
      "horror.\n",
      "\n",
      "I first drew the breath of life in Louisa County, Va., forty-five miles\n",
      "from the city of Richmond, in the year 1816. I was born a slave. Not\n",
      "because at the moment of my birth an angel stood by, and declared that\n",
      "such was the will of God concerning me; although in a country whose most\n",
      "honored writings declare that all men have a right to liberty, given\n",
      "them by their Creator, it seems strange that I, or any of my brethren,\n",
      "could have been born without this inalienable right, unless God had thus\n",
      "signified his departure from his usual rule, as described by our\n",
      "fathers. Not, I say, on account of God’s willing it to be so, was I born\n",
      "a slave, but for the reason that nearly all the people of this country\n",
      "are united in legislating against heaven, and have contrived to vote\n",
      "down our heavenly father’s rules, and to substitute for them, that cruel\n",
      "law which binds the chains of slavery upon one sixth part of the\n",
      "inhabitants of this land. I was born a slave! and wh\n"
     ]
    }
   ],
   "source": [
    "print(brown_fulltext[5000:6000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next code cell complete the following:\n",
    "\n",
    "- Look at the printed out \"slice\" of Brown's narrative. Make a new variable and assign it a value of **Brown's birth year.**\n",
    "- Make a new variable and assign it a value of: **how old Henry Brown would have been in the year 1860.**\n",
    "- Write a **print statement** using your new variable that says how old Henry Brown would have been in 1860.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Henry Brown was 44 in 1860\n"
     ]
    }
   ],
   "source": [
    "brown_birth=1816\n",
    "brown_age1860= 1860 - brown_birth\n",
    "print(f\"Henry Brown was {brown_age1860} in 1860\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to compare how long each narrative is measured by the number of lines in each text. First, use the `split()` function for each narrative to break it apart by each new line. The new line character is `\\n`. Make two new variables storing a list of the broken apart text: `truth_lines` and `brown_lines`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "truth_lines= truth_fulltext.split('\\n')\n",
    "brown_lines= brown_fulltext.split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which narrative has more lines? You can calculate how many lines are in each narrative through the `len()` function which will calculate the **length** of each list of lines you made in the previous section.\n",
    "\n",
    "- Write two print() statements to show **how many lines are in each narrative**.\n",
    "- Add a third print() statement that calculates **the difference between these two narratives measured by their number of lines**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3627 lines in truth.\n",
      "There are 2223 lines in brown.\n",
      "truth have 1404 more lines than brown\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(truth_lines)} lines in truth.\")\n",
    "print(f\"There are {len(brown_lines)} lines in brown.\")\n",
    "print(f\"truth have {len(truth_lines)-len(brown_lines)} more lines than brown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the `len()` and `comparison` functions with an `if statement` to print either `Sojourner Truth's narrative has more lines` or `Henry Brown's narrative has more lines` based on which has more lines.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sojourner Truth's narrative has more lines\n"
     ]
    }
   ],
   "source": [
    "if  len(truth_lines)> len(brown_lines):\n",
    " print(\"Sojourner Truth's narrative has more lines\")\n",
    "elif  len(truth_lines)== len(brown_lines):\n",
    " print(\"equal\")\n",
    "else :\n",
    " print(\"Henry Brown's narrative has more lines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting Word Frequency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the code below from Melanie Walsh's \"Anatomy of a Python Script\" that she used to calculate the most frequently occurring words in a novel \"The Yellow Wallpapper.\" You are going to use this code as a starting point but change it to apply this same approach to the two texts we've been working with. Your goal: **compare the most frequently occuring words in both Truth's narrative and Brown's narrative.**\n",
    "\n",
    "Note: don't edit Walsh's code cell directly. Instead, copy and paste the code into **the two empty code cells below it** that you can then edit. If you accidentally overwrite it and need to find the original, you can [copy it from the original tutorial.](https://melaniewalsh.github.io/Intro-Cultural-Analytics/02-Python/03-Anatomy-Python-Script.html)\n",
    "\n",
    "Adjustments you'll need to make to Walsh's code:\n",
    "\n",
    "- Open the right .txt file.\n",
    "- Find the most frequent **20 words** instead of 40 words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'The-Yellow-Wallpaper_Charlotte-Perkins-Gilman.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 26\u001b[0m\n\u001b[0;32m     11\u001b[0m number_of_desired_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m40\u001b[39m\n\u001b[0;32m     13\u001b[0m stopwords \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mme\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmyself\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwe\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mour\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mours\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mourselves\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myou\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myour\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myours\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     14\u001b[0m  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myourself\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myourselves\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhe\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhim\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhis\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhimself\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshe\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mher\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhers\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     15\u001b[0m  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mherself\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mit\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mits\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitself\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthey\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthem\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtheir\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtheirs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthemselves\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmost\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mother\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msome\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msuch\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnor\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124monly\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mown\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mso\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     24\u001b[0m  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthan\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoo\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvery\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwill\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjust\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdon\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshould\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnow\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mve\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mll\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwould\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mone\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 26\u001b[0m full_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(filepath_of_text, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     28\u001b[0m all_the_words \u001b[38;5;241m=\u001b[39m split_into_words(full_text)\n\u001b[0;32m     29\u001b[0m meaningful_words \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m all_the_words \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stopwords]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'The-Yellow-Wallpaper_Charlotte-Perkins-Gilman.txt'"
     ]
    }
   ],
   "source": [
    "#Walsh's Code - copy this into a new code cell\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def split_into_words(any_chunk_of_text):\n",
    "    lowercase_text = any_chunk_of_text.lower()\n",
    "    split_words = re.split(r\"\\W+\", lowercase_text)\n",
    "    return split_words\n",
    "\n",
    "filepath_of_text = \"The-Yellow-Wallpaper_Charlotte-Perkins-Gilman.txt\"\n",
    "number_of_desired_words = 40\n",
    "\n",
    "stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    " 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    " 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    " 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    " 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    " 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    " 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    " 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    " 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    " 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    " 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    " 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 've', 'll', 'amp', 'would', 'one']\n",
    "\n",
    "full_text = open(filepath_of_text, encoding=\"utf-8\").read()\n",
    "\n",
    "all_the_words = split_into_words(full_text)\n",
    "meaningful_words = [word for word in all_the_words if word not in stopwords]\n",
    "meaningful_words_tally = Counter(meaningful_words)\n",
    "most_frequent_meaningful_words = meaningful_words_tally.most_common(number_of_desired_words)\n",
    "\n",
    "most_frequent_meaningful_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('god', 128),\n",
       " ('isabella', 114),\n",
       " ('time', 114),\n",
       " ('could', 104),\n",
       " ('master', 78),\n",
       " ('go', 69),\n",
       " ('good', 67),\n",
       " ('said', 67),\n",
       " ('mr', 67),\n",
       " ('mother', 65),\n",
       " ('see', 64),\n",
       " ('much', 57),\n",
       " ('found', 53),\n",
       " ('like', 51),\n",
       " ('never', 50),\n",
       " ('well', 50),\n",
       " ('place', 50),\n",
       " ('son', 49),\n",
       " ('little', 49),\n",
       " ('new', 48)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def split_into_words(any_chunk_of_text):\n",
    "    lowercase_text = any_chunk_of_text.lower()\n",
    "    split_words = re.split(r\"\\W+\", lowercase_text)\n",
    "    return split_words\n",
    "\n",
    "filepath_of_text = \"truth.txt\"\n",
    "number_of_desired_words = 20\n",
    "\n",
    "stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    " 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    " 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    " 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    " 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    " 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    " 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    " 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    " 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    " 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    " 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    " 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 've', 'll', 'amp', 'would', 'one']\n",
    "\n",
    "full_text = open(filepath_of_text, encoding=\"utf-8\").read()\n",
    "\n",
    "all_the_words = split_into_words(full_text)\n",
    "meaningful_words = [word for word in all_the_words if word not in stopwords]\n",
    "meaningful_words_tally = Counter(meaningful_words)\n",
    "most_frequent_meaningful_words = meaningful_words_tally.most_common(number_of_desired_words)\n",
    "\n",
    "most_frequent_meaningful_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('man', 86),\n",
       " ('slave', 85),\n",
       " ('slavery', 83),\n",
       " ('master', 81),\n",
       " ('upon', 80),\n",
       " ('slaves', 75),\n",
       " ('us', 62),\n",
       " ('god', 52),\n",
       " ('could', 52),\n",
       " ('people', 49),\n",
       " ('time', 48),\n",
       " ('south', 43),\n",
       " ('may', 43),\n",
       " ('wife', 38),\n",
       " ('men', 37),\n",
       " ('government', 33),\n",
       " ('yet', 31),\n",
       " ('made', 31),\n",
       " ('must', 31),\n",
       " ('never', 30)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def split_into_words(any_chunk_of_text):\n",
    "    lowercase_text = any_chunk_of_text.lower()\n",
    "    split_words = re.split(r\"\\W+\", lowercase_text)\n",
    "    return split_words\n",
    "\n",
    "filepath_of_text = \"brown.txt\"\n",
    "number_of_desired_words = 20\n",
    "\n",
    "stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    " 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    " 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    " 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    " 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    " 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    " 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    " 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    " 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    " 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    " 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    " 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 've', 'll', 'amp', 'would', 'one']\n",
    "\n",
    "full_text = open(filepath_of_text, encoding=\"utf-8\").read()\n",
    "\n",
    "all_the_words = split_into_words(full_text)\n",
    "meaningful_words = [word for word in all_the_words if word not in stopwords]\n",
    "meaningful_words_tally = Counter(meaningful_words)\n",
    "most_frequent_meaningful_words = meaningful_words_tally.most_common(number_of_desired_words)\n",
    "\n",
    "most_frequent_meaningful_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the 20 most frequent words for each narrative. In the Markdown cell below, write down **three observations you have about this data.** These might be similarities between the two narratives, differences between the two, or any other patterns or questions you notice based on their word frequency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brown's text uses 'slave' and 'slavery' frequently, but Truth not.\n",
    "\n",
    "Both two text frequently use the word 'god'.\n",
    "\n",
    "Both two text frenquently use the word 'master'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text files you've used in this homework were not the original text files of these narratives. Instead, they've been cleaned by your instructor to make them shorter and easier to analyze. Your goal is to use Python to download the original `.txt` files from the website Project Gutenberg. Adapt the code from [these examples](https://python.omics.wiki/www/download-webpage) and use Python's `urllib` package to download the narratives and save them as local files named `truth-original.txt` and `brown-original.txt`.\n",
    "\n",
    "Here are the URL's for the two original text files on Project Gutenberg:\n",
    "\n",
    "- Truth's narrative: <https://www.gutenberg.org/cache/epub/1674/pg1674.txt>\n",
    "- Brown's narrative: <https://www.gutenberg.org/cache/epub/64992/pg64992.txt>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('brown-original.txt', <http.client.HTTPMessage at 0x1da967a7c80>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "urllib.request.urlretrieve('https://www.gutenberg.org/cache/epub/1674/pg1674.txt', 'truth-original.txt')\n",
    "import urllib.request\n",
    "\n",
    "urllib.request.urlretrieve('https://www.gutenberg.org/cache/epub/64992/pg64992.txt', 'brown-original.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write code for the following:\n",
    "\n",
    "- Open and read each of the new text files you just downloaded.\n",
    "- Print out the **number of lines** in each of the original (newly downloaded) text files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4087\n",
      "2814\n"
     ]
    }
   ],
   "source": [
    "truth_original= open('truth-original.txt', mode='r', encoding='utf-8').read()\n",
    "brown_original= open('brown-original.txt', mode='r', encoding='utf-8').read()\n",
    "truth_original_lines=truth_original.split('\\n')\n",
    "brown_original_lines=brown_original.split('\\n')\n",
    "print(len(truth_original_lines))\n",
    "print(len(brown_original_lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the length of the original text files you just downloaded to the cleaned text files you used for the rest of the homework, measured by the number of lines.\n",
    "\n",
    "Write two print() statements that calculate **how many lines were removed by the instructor for each narrative.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "truth= 460\n",
      "brown= 591\n"
     ]
    }
   ],
   "source": [
    "print(f\"truth= {len(truth_original_lines)-len(truth_lines)}\")\n",
    "print(f\"brown= {len(brown_original_lines)-len(brown_lines)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What sort of text did the instructor remove? Write Python code that allows you to compare the two versions Sojourner Truth's narrative. Then write a few sentences in the empty Markdown cell below explaining what you found.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿The Project Gutenberg eBook of The Narrative of Sojourner Truth\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost no restrictions\n",
      "whatsoever. You may copy it, give it away or re-use it under the terms\n",
      "of the Project Gutenberg License included with this ebook or online\n",
      "at www.gutenberg.org. If you are not located in the United States,\n",
      "you will have to check the laws of the country where you are located\n",
      "before using this eBook.\n",
      "\n",
      "Title: The Narrative of Sojourner Truth\n",
      "\n",
      "Author: Olive Gilbert\n",
      "        Sojourner Truth\n",
      "\n",
      "Release date: March 1, 1999 [eBook #1674]\n",
      "                Most recently updated: June 20, 2015\n",
      "\n",
      "Language: English\n",
      "\n",
      "Credits: This book is put on-line as part of the BUILD-A-BOOK Initiative at the Celebration of Women Writers through the combined work of Laura LeVine, Margaret Sylvia, and Mary Mark Ockerbloom\n",
      "\n",
      "\n",
      "*** START OF THE PROJECT GUTENBERG EBOOK THE NARRATIVE OF SOJOURNER TRUTH ***\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "This\n"
     ]
    }
   ],
   "source": [
    "print(truth_original[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NARRATIVE OF SOJOURNER TRUTH\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "HER BIRTH AND PARENTAGE.\n",
      "\n",
      "\n",
      "THE subject of this biography, SOJOURNER TRUTH, as she now calls\n",
      "herself-but whose name, originally, was Isabella-was born, as near as\n",
      "she can now calculate, between the years 1797 and 1800.  She was the\n",
      "daughter of James and Betsey, slaves of one Colonel Ardinburgh, Hurley,\n",
      "Ulster County, New York.\n",
      "\n",
      "Colonel  Ardinburgh belonged to that class of people called Low Dutch.\n",
      "\n",
      "\n",
      "Of her first master, she can give no account, as she must have been a\n",
      "mere infant when he died; and she, with her parents and some ten or\n",
      "twelve other fellow human chattels, became the legal property of his\n",
      "son, Charles Ardinburgh.  She distinctly remembers hearing her father\n",
      "and mother say, that their lot was a fortunate one, as Master Charles\n",
      "was the best of the family,-being, comparatively speaking, a kind\n",
      "master to his slaves.\n",
      "\n",
      "\n",
      "James and Betsey having, by their faithfulness, docility, and\n",
      "respectful behavior, won his particular regard, received from him\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(truth_fulltext[0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The revised version has removed the copyright description from the original version, and the information about the book has been removed, and I presume some information about the non-article content at the end of the book has also been removed, leaving us with a pure text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
